{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/02 13:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/02 13:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Seed Clustering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+\n",
      "| area|perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+\n",
      "|15.26|    14.84|      0.871|           5.763|          3.312|                2.221|                   5.22|\n",
      "|14.88|    14.57|     0.8811|           5.554|          3.333|                1.018|                  4.956|\n",
      "|14.29|    14.09|      0.905|           5.291|          3.337|                2.699|                  4.825|\n",
      "|13.84|    13.94|     0.8955|           5.324|          3.379|                2.259|                  4.805|\n",
      "|16.14|    14.99|     0.9034|           5.658|          3.562|                1.355|                  5.175|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('seeds.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length of kernel: double (nullable = true)\n",
      " |-- width of kernel: double (nullable = true)\n",
      " |-- asymmetry coefficient: double (nullable = true)\n",
      " |-- length of kernel groove: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df.columns\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 13:11:44 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(k=3, featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "model = kmeans.fit(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Silhouette Score is a metric used to calculate the goodness of a clustering technique, such as K-means clustering. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score =  0.6632174368466238\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette Score = \", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best parameters for K-means in PySpark involves performing a hyperparameter search, commonly known as hyperparameter tuning. You can use techniques like grid search or random search to explore different combinations of hyperparameters and find the ones that result in the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "K: 2\n",
      "Max Iterations: 10\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans()\n",
    "\n",
    "param_grid = (ParamGridBuilder().addGrid(kmeans.k, [2,3,4,5]).addGrid(kmeans.maxIter, [10,20,30,40]).build())\n",
    "evaluator = ClusteringEvaluator()\n",
    "crossval = CrossValidator(estimator=kmeans,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cv_model = crossval.fit(df_transformed)\n",
    "\n",
    "best_kmeans_model = cv_model.bestModel\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"K: {best_kmeans_model.getK()}\")\n",
    "print(f\"Max Iterations: {best_kmeans_model.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=2, maxIter=10, seed=42, featuresCol=\"features\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------------------------------+----------+\n",
      "|area |perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|features                                    |prediction|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------------------------------+----------+\n",
      "|15.26|14.84    |0.871      |5.763           |3.312          |2.221                |5.22                   |[15.26,14.84,0.871,5.763,3.312,2.221,5.22]  |1         |\n",
      "|14.88|14.57    |0.8811     |5.554           |3.333          |1.018                |4.956                  |[14.88,14.57,0.8811,5.554,3.333,1.018,4.956]|1         |\n",
      "|14.29|14.09    |0.905      |5.291           |3.337          |2.699                |4.825                  |[14.29,14.09,0.905,5.291,3.337,2.699,4.825] |1         |\n",
      "|13.84|13.94    |0.8955     |5.324           |3.379          |2.259                |4.805                  |[13.84,13.94,0.8955,5.324,3.379,2.259,4.805]|1         |\n",
      "|16.14|14.99    |0.9034     |5.658           |3.562          |1.355                |5.175                  |[16.14,14.99,0.9034,5.658,3.562,1.355,5.175]|0         |\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  134|\n",
      "|         0|   76|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score =  0.7161978322145462\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette Score = \", silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Silhouette Score:\n",
    "\n",
    "##### Close to +1: Indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "##### Close to 0: Indicates that the object is on or very close to the decision boundary between two neighboring clusters.\n",
    "\n",
    "##### Close to -1: Indicates that the object may be assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: By performing hyperparameter search, we got a higher Silhouette Score indicating close to +1 which means the model is performing well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
