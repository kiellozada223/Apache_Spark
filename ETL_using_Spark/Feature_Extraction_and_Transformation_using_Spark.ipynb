{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/02 17:13:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |sentences                                    |\n",
      "+---+---------------------------------------------+\n",
      "|1  |Spark is a distributed computing system      |\n",
      "|2  |It provides interfaces for multiple languages|\n",
      "|3  |Spark is built on top of Hadoop              |\n",
      "+---+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_dataframe = spark.createDataFrame([\n",
    "    (1, \"Spark is a distributed computing system\"),\n",
    "    (2, \"It provides interfaces for multiple languages\"),\n",
    "    (3, \"Spark is built on top of Hadoop\")\n",
    "], [\"id\", \"sentences\"])\n",
    "\n",
    "sentence_dataframe.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|id |sentences                                    |words                                               |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|1  |Spark is a distributed computing system      |[spark, is, a, distributed, computing, system]      |\n",
      "|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n",
      "|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_df = tokenizer.transform(sentence_dataframe)\n",
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+\n",
      "|id |words                                            |\n",
      "+---+-------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|\n",
      "+---+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdata = [(1, \"I love Spark Spark provides Python API\".split()),\n",
    "            (2, \"I love Python Spark supports Python\".split()),\n",
    "            (3, \"Spark solves the big problem of big data\".split())]\n",
    "\n",
    "textdata = spark.createDataFrame(textdata, [\"id\",\"words\"])\n",
    "textdata.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "model = cv.fit(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+---------------------------------------------------+\n",
      "|id |words                                            |features                                           |\n",
      "+---+-------------------------------------------------+---------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,3,8,10],[2.0,1.0,1.0,1.0,1.0,1.0])      |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,3,11],[1.0,2.0,1.0,1.0,1.0])            |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,4,5,6,7,9,12],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+---+-------------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(textdata)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "|id |sentence             |\n",
      "+---+---------------------+\n",
      "|1  |Spark supports python|\n",
      "|2  |Spark is fast        |\n",
      "|3  |Spark is easy        |\n",
      "+---+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_data = spark.createDataFrame([\n",
    "    (1, \"Spark supports python\"),\n",
    "    (2, \"Spark is fast\"),\n",
    "    (3, \"Spark is easy\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "sentence_data.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------+\n",
      "|id |sentence             |words                    |\n",
      "+---+---------------------+-------------------------+\n",
      "|1  |Spark supports python|[spark, supports, python]|\n",
      "|2  |Spark is fast        |[spark, is, fast]        |\n",
      "|3  |Spark is easy        |[spark, is, easy]        |\n",
      "+---+---------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "words_data = tokenizer.transform(sentence_data)\n",
    "words_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------+--------------------------+\n",
      "|id |sentence             |words                    |raw_features              |\n",
      "+---+---------------------+-------------------------+--------------------------+\n",
      "|1  |Spark supports python|[spark, supports, python]|(10,[4,6,9],[1.0,1.0,1.0])|\n",
      "|2  |Spark is fast        |[spark, is, fast]        |(10,[3,6,9],[1.0,1.0,1.0])|\n",
      "|3  |Spark is easy        |[spark, is, easy]        |(10,[0,6,9],[1.0,1.0,1.0])|\n",
      "+---+---------------------+-------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol='words', outputCol='raw_features', numFeatures=10)\n",
    "featurized_data = hashingTF.transform(words_data)\n",
    "featurized_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "idf_model = idf.fit(featurized_data)\n",
    "tfid_model = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------------------+\n",
      "|sentence             |features                                 |\n",
      "+---------------------+-----------------------------------------+\n",
      "|Spark supports python|(10,[4,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "|Spark is fast        |(10,[3,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "|Spark is easy        |(10,[0,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "+---------------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfid_model.select('sentence','features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+\n",
      "|id |sentence                                                    |\n",
      "+---+------------------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n",
      "+---+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textData = spark.createDataFrame([\n",
    "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
    "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
    "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                                    |filtered_sentence                                   |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol='sentence', outputCol='filtered_sentence')\n",
    "textData = remover.transform(textData)\n",
    "textData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| color|\n",
      "+---+------+\n",
      "|  0|   red|\n",
      "|  1|   red|\n",
      "|  2|  blue|\n",
      "|  3|yellow|\n",
      "|  4|yellow|\n",
      "|  5|yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "colors = spark.createDataFrame(\n",
    "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
    "    [\"id\", \"color\"])\n",
    "\n",
    "colors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id| color|color_index|\n",
      "+---+------+-----------+\n",
      "|  0|   red|        1.0|\n",
      "|  1|   red|        1.0|\n",
      "|  2|  blue|        2.0|\n",
      "|  3|yellow|        0.0|\n",
      "|  4|yellow|        0.0|\n",
      "|  5|yellow|        0.0|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='color', outputCol='color_index')\n",
    "color_indexed = indexer.fit(colors).transform(colors)\n",
    "color_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1| [70.0,170.0,17.0]|\n",
      "|  2| [80.0,165.0,25.0]|\n",
      "|  3|[65.0,150.0,135.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, Vectors.dense([70, 170, 17])),\n",
    "        (2, Vectors.dense([80, 165, 25])),\n",
    "        (3, Vectors.dense([65, 150, 135]))]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------------------------------+\n",
      "|id |features          |scaled_features                                            |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254367,-0.6369487984517485]|\n",
      "|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.3202563076101752,-0.5156252177942725]|\n",
      "|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021] |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "scaled_df = scaler_model.transform(df)\n",
    "scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Feature Extraction and Traformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------+\n",
      "|id |text                                                       |\n",
      "+---+-----------------------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.                          |\n",
      "|2  |Do not judge a book by its cover.                          |\n",
      "|3  |Actions speak louder than words.                           |\n",
      "|4  |A picture is worth a thousand words.                       |\n",
      "|5  |If at first you do not succeed try try again.              |\n",
      "|6  |Practice makes perfect.                                    |\n",
      "|7  |An apple a day keeps the doctor away.                      |\n",
      "|8  |When the going gets tough the tough get going.             |\n",
      "|9  |All is fair in love and war.                               |\n",
      "|10 |Too many cooks spoil the broth.                            |\n",
      "|11 |You can not make an omelette without breaking eggs.        |\n",
      "|12 |The early bird catches the worm.                           |\n",
      "|13 |Better late than never.                                    |\n",
      "|14 |Honesty is the best policy.                                |\n",
      "|15 |A penny saved is a penny earned.                           |\n",
      "|16 |Two wrongs do not make a right.                            |\n",
      "|17 |The grass is always greener on the other side of the fence.|\n",
      "|18 |Do not count your chickens before they're hatched.         |\n",
      "|19 |Laughter is the best medicine.                             |\n",
      "|20 |Rome wasn't built in a day.                                |\n",
      "+---+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data = spark.read.csv(\"proverbs.csv\", header=True, inferSchema=True)\n",
    "text_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|\n",
      "|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|\n",
      "|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|\n",
      "|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|\n",
      "|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg_data = spark.read.csv(\"mpg.csv\", header=True, inferSchema= True)\n",
    "mpg_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to tokenize the \"text\" column of the \"textdata\" dataframe and store the tokens in the column \"words\"\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "text_data = tokenizer.transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------+\n",
      "|id |words                                                                   |\n",
      "+---+------------------------------------------------------------------------+\n",
      "|1  |[when, in, rome, do, as, the, romans, do.]                              |\n",
      "|2  |[do, not, judge, a, book, by, its, cover.]                              |\n",
      "|3  |[actions, speak, louder, than, words.]                                  |\n",
      "|4  |[a, picture, is, worth, a, thousand, words.]                            |\n",
      "|5  |[if, at, first, you, do, not, succeed, try, try, again.]                |\n",
      "|6  |[practice, makes, perfect.]                                             |\n",
      "|7  |[an, apple, a, day, keeps, the, doctor, away.]                          |\n",
      "|8  |[when, the, going, gets, tough, the, tough, get, going.]                |\n",
      "|9  |[all, is, fair, in, love, and, war.]                                    |\n",
      "|10 |[too, many, cooks, spoil, the, broth.]                                  |\n",
      "|11 |[you, can, not, make, an, omelette, without, breaking, eggs.]           |\n",
      "|12 |[the, early, bird, catches, the, worm.]                                 |\n",
      "|13 |[better, late, than, never.]                                            |\n",
      "|14 |[honesty, is, the, best, policy.]                                       |\n",
      "|15 |[a, penny, saved, is, a, penny, earned.]                                |\n",
      "|16 |[two, wrongs, do, not, make, a, right.]                                 |\n",
      "|17 |[the, grass, is, always, greener, on, the, other, side, of, the, fence.]|\n",
      "|18 |[do, not, count, your, chickens, before, they're, hatched.]             |\n",
      "|19 |[laughter, is, the, best, medicine.]                                    |\n",
      "|20 |[rome, wasn't, built, in, a, day.]                                      |\n",
      "+---+------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data.select(\"id\", \"words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "model = cv.fit(text_data)\n",
    "text_data = model.transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |features                                                                    |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |(99,[0,4,5,11,12,41,69,93],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |\n",
      "|2  |(99,[1,3,4,19,20,31,44,54],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |\n",
      "|3  |(99,[7,10,81,86,97],[1.0,1.0,1.0,1.0,1.0])                                  |\n",
      "|4  |(99,[1,2,10,70,77,87],[2.0,1.0,1.0,1.0,1.0,1.0])                            |\n",
      "|5  |(99,[3,4,16,17,22,35,53,62,64],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|6  |(99,[24,27,34],[1.0,1.0,1.0])                                               |\n",
      "|7  |(99,[0,1,13,48,57,60,63,89],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
      "|8  |(99,[0,6,8,11,23,42,85],[2.0,1.0,2.0,1.0,1.0,1.0,1.0])                      |\n",
      "|9  |(99,[2,5,37,39,61,68,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                     |\n",
      "|10 |(99,[0,29,33,36,40,83],[1.0,1.0,1.0,1.0,1.0,1.0])                           |\n",
      "|11 |(99,[3,9,13,17,26,38,52,75,91],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|12 |(99,[0,47,51,65,73],[2.0,1.0,1.0,1.0,1.0])                                  |\n",
      "|13 |(99,[7,28,46,49],[1.0,1.0,1.0,1.0])                                         |\n",
      "|14 |(99,[0,2,15,43,98],[1.0,1.0,1.0,1.0,1.0])                                   |\n",
      "|15 |(99,[1,2,14,18,74],[2.0,1.0,2.0,1.0,1.0])                                   |\n",
      "|16 |(99,[1,3,4,9,50,78,79],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |\n",
      "|17 |(99,[0,2,25,55,56,71,80,88,92,95],[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|18 |(99,[3,4,30,45,59,67,72,94],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
      "|19 |(99,[0,2,15,32,84],[1.0,1.0,1.0,1.0,1.0])                                   |\n",
      "|20 |(99,[1,5,12,21,58,76],[1.0,1.0,1.0,1.0,1.0,1.0])                            |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data.select('id','features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"Origin\", outputCol=\"origin_index\")\n",
    "\n",
    "mpg_transformed = indexer.fit(mpg_data).transform(mpg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+------------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|origin_index|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+------------+\n",
      "|22.0|        4|      108.0|        94|  2379|      16.5|  73|Japanese|         1.0|\n",
      "|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|         0.0|\n",
      "|16.2|        6|      163.0|       133|  3410|      15.8|  78|European|         2.0|\n",
      "|15.0|        6|      250.0|        72|  3432|      21.0|  75|American|         0.0|\n",
      "|20.2|        6|      200.0|        88|  3060|      17.1|  81|American|         0.0|\n",
      "|21.0|        6|      155.0|       107|  2472|      14.0|  73|American|         0.0|\n",
      "|14.0|        8|      351.0|       153|  4129|      13.0|  72|American|         0.0|\n",
      "|14.0|        8|      350.0|       165|  4209|      12.0|  71|American|         0.0|\n",
      "|19.4|        8|      318.0|       140|  3735|      13.2|  78|American|         0.0|\n",
      "|30.5|        4|       97.0|        78|  2190|      14.1|  77|European|         2.0|\n",
      "|22.4|        6|      231.0|       110|  3415|      15.8|  81|American|         0.0|\n",
      "|29.5|        4|       97.0|        71|  1825|      12.2|  76|European|         2.0|\n",
      "|23.0|        4|       97.0|        54|  2254|      23.5|  72|European|         2.0|\n",
      "|32.1|        4|       98.0|        70|  2120|      15.5|  80|American|         0.0|\n",
      "|38.0|        4|      105.0|        63|  2125|      14.7|  82|American|         0.0|\n",
      "|21.1|        4|      134.0|        95|  2515|      14.8|  78|Japanese|         1.0|\n",
      "|16.0|        6|      250.0|       100|  3781|      17.0|  74|American|         0.0|\n",
      "|30.0|        4|      111.0|        80|  2155|      14.8|  77|American|         0.0|\n",
      "|35.0|        4|       72.0|        69|  1613|      18.0|  71|Japanese|         1.0|\n",
      "|17.0|        8|      304.0|       150|  3672|      11.5|  72|American|         0.0|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "mpg_transformed.orderBy(rand()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| MPG|            features|\n",
      "+----+--------------------+\n",
      "|15.0|[8.0,390.0,190.0,...|\n",
      "|21.0|[6.0,199.0,90.0,2...|\n",
      "|18.0|[6.0,199.0,97.0,2...|\n",
      "|16.0|[8.0,304.0,150.0,...|\n",
      "|14.0|[8.0,455.0,225.0,...|\n",
      "|15.0|[8.0,350.0,165.0,...|\n",
      "|18.0|[8.0,307.0,130.0,...|\n",
      "|14.0|[8.0,454.0,220.0,...|\n",
      "|15.0|[8.0,400.0,150.0,...|\n",
      "|10.0|[8.0,307.0,200.0,...|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Cylinders\", \"Engine Disp\", \"Horsepower\", \"Weight\"],\n",
    "                            outputCol=\"features\")\n",
    "mpg_transformed = assembler.transform(mpg_transformed)\n",
    "\n",
    "mpg_transformed.select('MPG', 'features').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "scaled_mpg_transformed = scaler.fit(mpg_transformed).transform(mpg_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------------------------------------------------------+\n",
      "|features                |scaled_features                                                                    |\n",
      "+------------------------+-----------------------------------------------------------------------------------+\n",
      "|[8.0,390.0,190.0,3850.0]|[1.48205302652896,1.869079955831451,2.222084561602166,1.027093462353608]           |\n",
      "|[6.0,199.0,90.0,2648.0] |[0.3095711165403583,0.043843985634147174,-0.37591456792553746,-0.38801882543985255]|\n",
      "|[6.0,199.0,97.0,2774.0] |[0.3095711165403583,0.043843985634147174,-0.1940546288585982,-0.2396792678175763]  |\n",
      "|[8.0,304.0,150.0,3433.0]|[1.48205302652896,1.0472459587792617,1.1828849097910845,0.5361601645084557]        |\n",
      "|[8.0,455.0,225.0,3086.0]|[1.48205302652896,2.4902335582546176,3.131384256936862,0.12763773200901246]        |\n",
      "|[8.0,350.0,165.0,3693.0]|[1.48205302652896,1.4868315851095026,1.57258477922024,0.8422576643639463]          |\n",
      "|[8.0,307.0,130.0,3504.0]|[1.48205302652896,1.0759145865834079,0.6632850838855439,0.619748327930532]         |\n",
      "|[8.0,454.0,220.0,4354.0]|[1.48205302652896,2.480677348986569,3.001484300460477,1.6204516928427128]          |\n",
      "|[8.0,400.0,150.0,3761.0]|[1.48205302652896,1.964642048511938,1.1828849097910845,0.9223139335569208]         |\n",
      "|[8.0,307.0,200.0,4376.0]|[1.48205302652896,1.0759145865834079,2.481884474554936,1.646352250522793]          |\n",
      "|[8.0,383.0,170.0,3563.0]|[1.48205302652896,1.80218649095511,1.7024847356966253,0.689208914436201]           |\n",
      "|[8.0,318.0,210.0,4382.0]|[1.48205302652896,1.1810328885319439,2.7416843875077066,1.6534160389809966]        |\n",
      "|[8.0,360.0,215.0,4615.0]|[1.48205302652896,1.5823936777899896,2.871584343984092,1.9277264907745708]         |\n",
      "|[8.0,429.0,198.0,4341.0]|[1.48205302652896,2.241772117285351,2.4299244919643823,1.6051468178499384]         |\n",
      "|[6.0,200.0,85.0,2587.0] |[0.3095711165403583,0.053400194902195885,-0.5058145244019226,-0.4598340080982561]  |\n",
      "|[8.0,302.0,140.0,3449.0]|[1.48205302652896,1.0281335402431644,0.9230849968383142,0.5549969337303321]        |\n",
      "|[8.0,304.0,193.0,4732.0]|[1.48205302652896,1.0472459587792617,2.300024535487997,2.0654703657095417]         |\n",
      "|[8.0,340.0,160.0,3609.0]|[1.48205302652896,1.3912694924290154,1.442684822743855,0.7433646259490956]         |\n",
      "|[6.0,198.0,95.0,2833.0] |[0.3095711165403583,0.03428777636609846,-0.24601461144915227,-0.17021868131190726] |\n",
      "|[8.0,440.0,215.0,4312.0]|[1.48205302652896,2.3468904192338864,2.871584343984092,1.5710051736352875]         |\n",
      "+------------------------+-----------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_mpg_transformed.select('features', 'scaled_features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
